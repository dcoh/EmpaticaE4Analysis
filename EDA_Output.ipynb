{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should have all the same signal processing functions from the signal package in R:\n",
    "## https://cran.r-project.org/web/packages/signal/signal.pdf\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "## https://docs.scipy.org/doc/scipy/reference/signal.html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to convert from the unix timestamp to UTC and to add time to an input date based on a sample rate (will be 4hz for this code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utc_date_time(ts):\n",
    "    return pd.to_datetime(ts, unit='s', infer_datetime_format = True)\n",
    "#.strftime('%H:%M:%S:%f')\n",
    "\n",
    "def add_fs(sample_rate, date):\n",
    "    return date + datetime.timedelta(milliseconds=1.0/(sample_rate) * 1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found some code on stack overflow and in SciPy's community website that implements a band pass butterworth filter for a 1d array: https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html#.  I dont know what order filter we need for this data, but for exploratory data analysis, it appears to work ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "## May need to better understand which order filter is needed for this dataset\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, column_name, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    y = pd.DataFrame(y, columns=[column_name])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that will generate the list of timestamps that will be added as a new column to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_range(df_length, start_timestamp):\n",
    "    # Generate date time range based on sample rate and starting timestamp\n",
    "    time_range = []\n",
    "    t_0 = get_utc_date_time(float(start_timestamp))\n",
    "    time_range.append(t_0)\n",
    "    \n",
    "    next_date = t_0\n",
    "    \n",
    "    for count in range(df_length-1):\n",
    "        next_date = add_fs(sample_rate, next_date)\n",
    "        time_range.append(next_date)\n",
    "    return time_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that returns a list of floats iterated upwards by sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_sample_rate(df_length, fs):\n",
    "    # returns list of floats starting from zero iterated upward by sample rate\n",
    "    result = []\n",
    "    curr_time = 0.0\n",
    "    result.append(curr_time)\n",
    "    \n",
    "    for i in range(df_length-1):\n",
    "        curr_time += 1.0/fs\n",
    "        result.append(curr_time)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that finds the time difference between 2 unix timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_0_diff_seconds(t_0, date_time):\n",
    "    return (date_time - pd.to_datetime(t_0, unit='s', infer_datetime_format = True)).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change file_path to point to where the folders containing EDA data will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'/Users/dancohen/Dropbox/E4 stuff/test data' #/Users/dancohen/Dropbox/E4 stuff/test data'\n",
    "output_path = r'/Users/dancohen/Dropbox/E4 stuff/test data/EDA_All_Data' #/Users/dancohen/Dropbox/E4 stuff/test data/HR_All_Data'\n",
    "all_folders = glob.glob(file_path + \"/PRF*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA peak detection and stats about the found peaks.  Also can output graphs with the peaks labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_detection_stats(data, part_id):\n",
    "    peaks, features = find_peaks(data[part_id], height=(0.01, None))\n",
    "    peak_list = peaks.tolist()\n",
    "    # Uncomment the following lines to output graphs with peaks labeled\n",
    "    #plt.figure(figsize=(30, 7))\n",
    "    #plt.plot(data[part_id])\n",
    "    #plt.plot(data.iloc[peak_list],'x')\n",
    "    #plt.title(\"PRF{}\".format(part_id))\n",
    "    #plt.ylabel('EDA (microsiemens)')\n",
    "    \n",
    "    #plt.savefig(output_path+\"/EDA{}.pdf\".format(part_id))\n",
    "    #plt.show()\n",
    "    return eda_sum_stats(data.iloc[peak_list], part_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 4.0 ## Sampling frequency.  This test data was gathered at 4hz\n",
    "lowcut = 0.05 ## Lower bound for the filter, as directed by Empatica documentation\n",
    "highcut = 1.0  ## Upper bound for the filter, as directed by Empatica documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of file paths that each will contain the files we are interested in, we can start to do work with the csv files they contain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that calculates stats for EDA peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_sum_stats(peaks, part_id):\n",
    "    #no peaks were found in this section of the data\n",
    "    if(len(peaks)==0):\n",
    "        peak_max = None\n",
    "        peak_avg = None\n",
    "        peak_num = 0\n",
    "    else:    \n",
    "        np_peaks = np.array(peaks[part_id])\n",
    "        peak_max = np_peaks.max()\n",
    "        peak_avg = np_peaks.mean()\n",
    "        peak_num = len(peaks)\n",
    "    stats = {part_id: [peak_max, peak_avg, peak_num]}\n",
    "    names = ['Max Peak Height', 'Peak Height Avg', 'Number of Peaks']\n",
    "    df = pd.DataFrame(stats)\n",
    "    df['Participant Id'] = names\n",
    "    df = df.set_index('Participant Id')\n",
    "    return df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eda_list = []\n",
    "script_1_2 = []\n",
    "script_2_3 = []\n",
    "script_3_end = []\n",
    "\n",
    "# Keep track of statistics for each participant\n",
    "st_1_2 = []\n",
    "st_2_3 = []\n",
    "st_3_end = []\n",
    "\n",
    "for folder in all_folders:\n",
    "    spl = folder.split('/')\n",
    "    \n",
    "    #Folder names are 'PRF###' (participant ID) and we are interested in the ID\n",
    "    part_id = spl[len(spl)-1][3:]\n",
    "    \n",
    "    eda_df_raw = pd.read_csv(folder+\"/eda.csv\")\n",
    "    starting_timestamp = eda_df_raw.columns[0]\n",
    "    sample_rate = eda_df_raw[starting_timestamp][0]\n",
    "    \n",
    "    # Grab the csv containing the timestamps that mark the start and end times of the scripts\n",
    "    script_times = pd.read_csv(folder+\"/tags.csv\", header=None)\n",
    "    script_times = script_times.apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "    \n",
    "    try:\n",
    "        begin_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[1].iat[0])\n",
    "        end_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[2].iat[0])\n",
    "\n",
    "        begin_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[3].iat[0])    \n",
    "        end_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[4].iat[0])\n",
    "\n",
    "        begin_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[5].iat[0])\n",
    "        end_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[6].iat[0])\n",
    "    except Exception as err:\n",
    "        print(\"Error occurred parsing tags.csv for PRF{}.  Error: {}. This file will be skipped.\".format(part_id, err))\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Drop the first 2 rows, as we already have the timestamp and sample rate\n",
    "    eda_df_raw = eda_df_raw.drop(eda_df_raw.index[0:2])\n",
    "    \n",
    "    # Rename the column to the participant ID\n",
    "    eda_df_raw = eda_df_raw.rename(columns={starting_timestamp: part_id})\n",
    "    \n",
    "    # Generate data frame with timestamps iterating upwards starting at t_0\n",
    "    #timestamps = pd.DataFrame(time_range(len(eda_df_raw), starting_timestamp))\n",
    "    \n",
    "    # Generate list of timings starting from 0 iterated upwards by 1/sample rate seconds\n",
    "    time_col = iterate_sample_rate(len(eda_df_raw), sample_rate)\n",
    "    \n",
    "    eda_df_filtered = butter_bandpass_filter(eda_df_raw[part_id], lowcut, highcut, fs, part_id)\n",
    "    \n",
    "    # Now, we can parse out data that we are interested in.  We will get 3 dataframes, one with data between \n",
    "    # the end of script 1 and beginning of script 2, one between the end of script 2 and beginning of script 3\n",
    "    # and one that has the data from end of script 3 to the end of the timing\n",
    "    eda_df_times = eda_df_filtered.copy()\n",
    "        \n",
    "    eda_df_times['Timestamp'] = time_col\n",
    "    \n",
    "    df_1_2 = eda_df_times[(eda_df_times['Timestamp'] >= end_s1) & (eda_df_times['Timestamp'] < begin_s2)]\n",
    "    df_2_3 = eda_df_times[(eda_df_times['Timestamp'] >= end_s2) & (eda_df_times['Timestamp'] < begin_s3)]\n",
    "    df_3_end = eda_df_times[(eda_df_times['Timestamp'] >= end_s3)]\n",
    "    \n",
    "    #Normalize each frame's timestamp down to 0.00 seconds once we know where the timing cutoffs for each frame are\n",
    "    #See lines above\n",
    "    norm_1_2 = df_1_2.iloc[0][1]\n",
    "    #Make a copy here to prevent chaining assignment in the dataframe\n",
    "    cp_1_2 = df_1_2.copy()\n",
    "    cp_1_2['Timestamp'] = df_1_2.apply(lambda x: x['Timestamp']-norm_1_2, axis=1)\n",
    "    cp_1_2 = cp_1_2.set_index('Timestamp')\n",
    "    stats_1_2 = peak_detection_stats(cp_1_2, part_id)\n",
    "    \n",
    "    norm_2_3 = df_2_3.iloc[0][1]\n",
    "    cp_2_3 = df_2_3.copy()\n",
    "    cp_2_3['Timestamp'] = df_2_3.apply(lambda x: x['Timestamp']-norm_2_3, axis=1)\n",
    "    cp_2_3 = cp_2_3.set_index('Timestamp')\n",
    "    stats_2_3 = peak_detection_stats(cp_2_3, part_id)\n",
    "\n",
    "    \n",
    "    norm_3_end = df_3_end.iloc[0][1]\n",
    "    cp_3_end = df_3_end.copy()\n",
    "    cp_3_end['Timestamp'] = df_3_end.apply(lambda x: x['Timestamp']-norm_3_end, axis=1)\n",
    "    cp_3_end = cp_3_end.set_index('Timestamp')\n",
    "    stats_3_end = peak_detection_stats(cp_3_end, part_id)\n",
    "\n",
    "    \n",
    "    script_1_2.append(cp_1_2)\n",
    "    st_1_2.append(stats_1_2)\n",
    "    \n",
    "    script_2_3.append(cp_2_3)\n",
    "    st_2_3.append(stats_2_3)\n",
    "    \n",
    "    script_3_end.append(cp_3_end)\n",
    "    st_3_end.append(stats_3_end)\n",
    "    #Set the index to be the timestamp, so that when we concatenate the data together we get an easy join\n",
    "    eda_df_times = eda_df_times.set_index('Timestamp')\n",
    "    eda_list.append(eda_df_times.transpose())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_all_participants = pd.concat(eda_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>012</th>\n",
       "      <th>014</th>\n",
       "      <th>205</th>\n",
       "      <th>018</th>\n",
       "      <th>020</th>\n",
       "      <th>019</th>\n",
       "      <th>002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.035781</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.006139</td>\n",
       "      <td>0.001216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.224242</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.073250</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.012965</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>0.008277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.613521</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>0.188914</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.034941</td>\n",
       "      <td>0.093272</td>\n",
       "      <td>0.023869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.984608</td>\n",
       "      <td>0.036546</td>\n",
       "      <td>0.260583</td>\n",
       "      <td>0.041149</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.125543</td>\n",
       "      <td>0.037354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>1.104027</td>\n",
       "      <td>0.031341</td>\n",
       "      <td>0.197817</td>\n",
       "      <td>0.036149</td>\n",
       "      <td>0.043791</td>\n",
       "      <td>0.093314</td>\n",
       "      <td>0.033335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319.25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319.50</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319.75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320.00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9281 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                012       014       205       018       020       019  \\\n",
       "Timestamp                                                               \n",
       "0.00       0.035781  0.001383  0.011937  0.001493  0.002046  0.006139   \n",
       "0.25       0.224242  0.008917  0.073250  0.009730  0.012965  0.036958   \n",
       "0.50       0.613521  0.024450  0.188914  0.027044  0.034941  0.093272   \n",
       "0.75       0.984608  0.036546  0.260583  0.041149  0.051446  0.125543   \n",
       "1.00       1.104027  0.031341  0.197817  0.036149  0.043791  0.093314   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "2319.00         NaN       NaN       NaN -0.009843       NaN       NaN   \n",
       "2319.25         NaN       NaN       NaN -0.010008       NaN       NaN   \n",
       "2319.50         NaN       NaN       NaN -0.010840       NaN       NaN   \n",
       "2319.75         NaN       NaN       NaN -0.012225       NaN       NaN   \n",
       "2320.00         NaN       NaN       NaN -0.011294       NaN       NaN   \n",
       "\n",
       "                002  \n",
       "Timestamp            \n",
       "0.00       0.001216  \n",
       "0.25       0.008277  \n",
       "0.50       0.023869  \n",
       "0.75       0.037354  \n",
       "1.00       0.033335  \n",
       "...             ...  \n",
       "2319.00         NaN  \n",
       "2319.25         NaN  \n",
       "2319.50         NaN  \n",
       "2319.75         NaN  \n",
       "2320.00         NaN  \n",
       "\n",
       "[9281 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_all_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_all_participants.to_csv(output_path+\"/EDA_All_Data.csv\", float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we can output 3 separate files representing the script timings.  One for the data between end of script 1 and beginning of script 2, one for end of script 2 and beginning of script 3 and one for end of script 3 and end of the data.\n",
    "\n",
    "Use the axis=1 option to concatenate (defaulting to an outer join) along the timestamp axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1_2 = pd.concat(script_1_2, axis=1)\n",
    "output_2_3 = pd.concat(script_2_3, axis=1)\n",
    "output_3_end = pd.concat(script_3_end, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_1_2 = pd.concat(st_1_2)\n",
    "stat_2_3 = pd.concat(st_2_3)\n",
    "stat_3_end = pd.concat(st_3_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(output_3_end['002'])\n",
    "\n",
    "#peaks, features = find_peaks(ibi_df[part_id], height=(0.01, None))\n",
    "#peak_list = peaks.tolist()\n",
    "\n",
    "#plt.scatter(ibi_df.iloc[peak_list]['Time_After_T0'], ibi_df.iloc[peak_list][part_id], c='r')\n",
    "#plt.plot(output_1_2)\n",
    "#plt.plot(output_2_3)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1_2.to_csv(output_path+\"/EDA_1_2.csv\", float_format='%.6f')\n",
    "output_2_3.to_csv(output_path+\"/EDA_2_3.csv\", float_format='%.6f')\n",
    "output_3_end.to_csv(output_path+\"/EDA_3_end.csv\", float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_1_2.to_csv(output_path+\"/EDA_stats_1_2.csv\", float_format='%.6f')\n",
    "stat_2_3.to_csv(output_path+\"/EDA_stats_2_3.csv\", float_format='%.6f')\n",
    "stat_3_end.to_csv(output_path+\"/EDA_stats_3_end.csv\", float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
