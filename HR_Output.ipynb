{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should have all the same signal processing functions from the signal package in R:\n",
    "## https://cran.r-project.org/web/packages/signal/signal.pdf\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "## https://docs.scipy.org/doc/scipy/reference/signal.html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import heartpy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'/Users/dancohen/Dropbox/E4 stuff/test data'\n",
    "output_path = r'/Users/dancohen/Dropbox/E4 stuff/test data/HR_All_Data'\n",
    "all_folders = glob.glob(file_path + \"/PRF*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_sample_rate(df_length, fs):\n",
    "    # returns list of floats starting from zero iterated upward by sample rate\n",
    "    result = []\n",
    "    curr_time = 0.0\n",
    "    result.append(curr_time)\n",
    "    \n",
    "    for i in range(df_length-1):\n",
    "        curr_time += 1.0/fs\n",
    "        result.append(curr_time)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_0_diff_seconds(t_0, date_time):\n",
    "    return (date_time - pd.to_datetime(t_0, unit='s', infer_datetime_format = True)).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(start_df, col_label):\n",
    "    #Make a copy here to prevent chaining assignment in the dataframe\n",
    "    start_seconds = start_df.iloc[0][1]\n",
    "    cp = start_df.copy()\n",
    "    cp[col_label] = start_df.apply(lambda x: x[col_label]-start_seconds, axis=1)\n",
    "    cp = cp.set_index(col_label)\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [012]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [012]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [014]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [014]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [205]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [205]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [018]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [018]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [020]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [020]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [019]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [019]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [002]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [002]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "hr_all_part = []\n",
    "hr_1_2 = []\n",
    "hr_2_3 = []\n",
    "hr_3_end = []\n",
    "\n",
    "for folder in all_folders:\n",
    "    spl = folder.split('/')\n",
    "    \n",
    "    #Folder names are 'PRF###' (participant ID) and we are interested in the ID\n",
    "    part_id = spl[len(spl)-1][3:]\n",
    "    \n",
    "    hr_df_raw = pd.read_csv(folder+\"/HR.csv\")\n",
    "    starting_timestamp = hr_df_raw.columns[0]\n",
    "    sample_rate = hr_df_raw[starting_timestamp][0]\n",
    "    hr_df_raw = hr_df_raw.drop([0, 0])\n",
    "    time_col = iterate_sample_rate(len(hr_df_raw), sample_rate)\n",
    "    hr_df = hr_df_raw.copy()\n",
    "    hr_df['Timestamp'] = time_col\n",
    "    hr_df = hr_df.rename(columns={starting_timestamp:part_id})\n",
    "\n",
    "    \n",
    "    # Grab the csv containing the timestamps that mark the start and end times of the scripts\n",
    "    script_times = pd.read_csv(folder+\"/tags.csv\", header=None)\n",
    "    script_times = script_times.apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "\n",
    "    starting_timestamp = hr_df_raw.columns[0]\n",
    "    \n",
    "    try:\n",
    "        begin_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[1].iat[0])\n",
    "        end_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[2].iat[0])\n",
    "        \n",
    "        begin_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[3].iat[0])    \n",
    "        end_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[4].iat[0])\n",
    "\n",
    "        begin_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[5].iat[0])\n",
    "        end_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[6].iat[0])\n",
    "    except Exception as err:\n",
    "        print(\"Error occurred parsing tags.csv for PRF{}.  Error: {}. This file will be skipped.\".format(part_id, err))\n",
    "        continue\n",
    "        \n",
    "    df_1_2 = hr_df[(hr_df['Timestamp'] >= end_s1) & (hr_df['Timestamp'] < begin_s2)]\n",
    "    df_1_2 = normalize_df(df_1_2, 'Timestamp')\n",
    "\n",
    "    \n",
    "    df_2_3 = hr_df[(hr_df['Timestamp'] >= end_s2) & (hr_df['Timestamp'] < begin_s3)]\n",
    "    df_2_3 = normalize_df(df_2_3, 'Timestamp')\n",
    "\n",
    "    df_3_end = hr_df[(hr_df['Timestamp'] >= end_s3)]\n",
    "    df_3_end = normalize_df(df_3_end, 'Timestamp')\n",
    "\n",
    "    hr_df = hr_df.rename(columns={starting_timestamp: part_id})\n",
    "    hr_df = hr_df.set_index('Timestamp')\n",
    "\n",
    "    test = hr_df[hr_df[part_id]>200]\n",
    "    print(test)\n",
    "    \n",
    "    test = hr_df[hr_df[part_id]<25]\n",
    "    print(test)\n",
    "    \n",
    "    #plt.figure(figsize=(30, 7))\n",
    "    #plt.plot(time_col, hr_df[part_id] )\n",
    "    #plt.title(\"PRF{}\".format(part_id))\n",
    "    #plt.ylabel('HR')\n",
    "    #plt.xlabel('Time After T0 (seconds)')\n",
    "    #plt.savefig(output_path+\"/HR{}.pdf\".format(part_id))\n",
    "    #plt.show()\n",
    "    \n",
    "    hr_all_part.append(hr_df)\n",
    "    hr_1_2.append(df_1_2)\n",
    "    hr_2_3.append(df_2_3)\n",
    "    hr_3_end.append(df_3_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(hr_all_part, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2 = pd.concat(hr_1_2, axis=1)\n",
    "df_2_3 = pd.concat(hr_2_3, axis=1)\n",
    "df_3_end = pd.concat(hr_3_end, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2.to_csv(output_path+\"/HR_1_2.csv\", float_format='%.6f')\n",
    "df_2_3.to_csv(output_path+\"/HR_2_3.csv\", float_format='%.6f')\n",
    "df_3_end.to_csv(output_path+\"/HR_3_end.csv\", float_format='%.6f')\n",
    "result.to_csv(output_path+\"/HR_All_Data.csv\", float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
