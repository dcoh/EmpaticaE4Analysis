{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should have all the same signal processing functions from the signal package in R:\n",
    "## https://cran.r-project.org/web/packages/signal/signal.pdf\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "## https://docs.scipy.org/doc/scipy/reference/signal.html\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'/Users/dancohen/Dropbox/E4 stuff/raw data copy' #/Users/dancohen/Dropbox/E4 stuff/test data'\n",
    "output_path = r'/Users/dancohen/Dropbox/E4 stuff/Data_Output/HR_Data' #/Users/dancohen/Dropbox/E4 stuff/test data/HR_All_Data'\n",
    "all_folders = glob.glob(file_path + \"/PRF*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_sample_rate(df_length, fs):\n",
    "    # returns list of floats starting from zero iterated upward by sample rate\n",
    "    result = []\n",
    "    curr_time = 0.0\n",
    "    result.append(curr_time)\n",
    "    \n",
    "    for i in range(df_length-1):\n",
    "        curr_time += 1.0/fs\n",
    "        result.append(curr_time)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_0_diff_seconds(t_0, date_time):\n",
    "    return (date_time - pd.to_datetime(t_0, unit='s', infer_datetime_format = True)).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(start_df, col_label):\n",
    "    #Make a copy here to prevent chaining assignment in the dataframe\n",
    "    start_seconds = start_df.iloc[0][1]\n",
    "    cp = start_df.copy()\n",
    "    cp[col_label] = start_df.apply(lambda x: x[col_label]-start_seconds, axis=1)\n",
    "    cp = cp.set_index(col_label)\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return summary stats for HR data.  \n",
    "\n",
    "Includes: Average HR for first 30 samples, Average HR for last 30 samples, and diff between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_sum_stats(data_col, part_id):\n",
    "    first_30_avg = data_col[part_id].head(30).mean()\n",
    "    last_30_avg = data_col[part_id].tail(30).mean()\n",
    "    avg = data_col[part_id].mean()\n",
    "    start_end_diff = first_30_avg - last_30_avg\n",
    "    stats = {part_id: [first_30_avg, last_30_avg, avg, start_end_diff]}\n",
    "    names = ['First 30 Sec Avg', 'Final 30 Sec Avg', 'Script Avg', 'Diff of Avg']\n",
    "    df = pd.DataFrame(stats)\n",
    "    df['Participant ID'] = names\n",
    "    df = df.set_index('Participant ID')\n",
    "    return df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to output a file containing values that are less than 25 or greater than 200 for HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_ob_vals(data_col, part_id):\n",
    "    ob_hr = data_col[(data_col[part_id]<=25) | (data_col[part_id]>=200)]\n",
    "    data = {'Participant Id': [part_id] * len(ob_hr), 'HR': ob_hr[part_id], 'Timestamp': ob_hr['Timestamp']}\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index('Participant Id')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_all_part = []\n",
    "\n",
    "#HR stats should be run for each script, outputting 3 files\n",
    "hr_1_2 = []\n",
    "stats_1_2 = []\n",
    "\n",
    "hr_2_3 = []\n",
    "stats_2_3 = []\n",
    "\n",
    "hr_3_end = []\n",
    "stats_3_end = []\n",
    "\n",
    "hr_ob_vals = []\n",
    "\n",
    "for folder in all_folders:\n",
    "    spl = folder.split('/')\n",
    "    \n",
    "    #Folder names are 'PRF###' (participant ID) and we are interested in the ID\n",
    "    part_id = spl[len(spl)-1][3:]\n",
    "    \n",
    "    hr_df_raw = pd.read_csv(folder+\"/HR.csv\")\n",
    "    starting_timestamp = hr_df_raw.columns[0]\n",
    "    sample_rate = hr_df_raw[starting_timestamp][0]\n",
    "    hr_df_raw = hr_df_raw.drop([0, 0])\n",
    "    time_col = iterate_sample_rate(len(hr_df_raw), sample_rate)\n",
    "    hr_df = hr_df_raw.copy()\n",
    "    hr_df['Timestamp'] = time_col\n",
    "    hr_df = hr_df.rename(columns={starting_timestamp:part_id})\n",
    "\n",
    "    \n",
    "    # Grab the csv containing the timestamps that mark the start and end times of the scripts\n",
    "    script_times = pd.read_csv(folder+\"/tags.csv\", header=None)\n",
    "    script_times = script_times.apply(lambda x: pd.to_datetime(x, unit='s'))\n",
    "\n",
    "    starting_timestamp = hr_df_raw.columns[0]\n",
    "    \n",
    "    try:\n",
    "        begin_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[1].iat[0])\n",
    "        end_s1 = t_0_diff_seconds(starting_timestamp, script_times.loc[2].iat[0])\n",
    "        \n",
    "        begin_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[3].iat[0])    \n",
    "        end_s2 = t_0_diff_seconds(starting_timestamp, script_times.loc[4].iat[0])\n",
    "\n",
    "        begin_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[5].iat[0])\n",
    "        end_s3 = t_0_diff_seconds(starting_timestamp, script_times.loc[6].iat[0])\n",
    "    except Exception as err:\n",
    "        print(\"Error occurred parsing tags.csv for PRF{}.  Error: {}. This file will be skipped.\".format(part_id, err))\n",
    "        continue\n",
    "        \n",
    "    df_1_2 = hr_df[(hr_df['Timestamp'] >= end_s1) & (hr_df['Timestamp'] < begin_s2)]\n",
    "    df_1_2 = normalize_df(df_1_2, 'Timestamp')\n",
    "    stat_1_2 = hr_sum_stats(df_1_2, part_id)\n",
    "    \n",
    "    df_2_3 = hr_df[(hr_df['Timestamp'] >= end_s2) & (hr_df['Timestamp'] < begin_s3)]\n",
    "    df_2_3 = normalize_df(df_2_3, 'Timestamp')\n",
    "    stat_2_3 = hr_sum_stats(df_2_3, part_id)\n",
    "    \n",
    "    df_3_end = hr_df[(hr_df['Timestamp'] >= end_s3)]\n",
    "    df_3_end = normalize_df(df_3_end, 'Timestamp')\n",
    "    stat_3_end = hr_sum_stats(df_1_2, part_id)\n",
    "\n",
    "    hr_df = hr_df.rename(columns={starting_timestamp: part_id})\n",
    "    #Output summary stats for HR for each participant, sort by participant ID\n",
    "    ob_vals = output_ob_vals(hr_df, part_id)\n",
    "    hr_df = hr_df.set_index('Timestamp')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plt.figure(figsize=(30, 7))\n",
    "    #plt.plot(time_col, hr_df[part_id] )\n",
    "    #plt.title(\"PRF{}\".format(part_id))\n",
    "    #plt.ylabel('HR')\n",
    "    #plt.xlabel('Time After T0 (seconds)')\n",
    "    #plt.savefig(output_path+\"/HR{}.pdf\".format(part_id))\n",
    "    #plt.show()\n",
    "    \n",
    "    hr_ob_vals.append(ob_vals)\n",
    "    hr_all_part.append(hr_df)\n",
    "    \n",
    "    hr_1_2.append(df_1_2)\n",
    "    stats_1_2.append(stat_1_2)\n",
    "    \n",
    "    hr_2_3.append(df_2_3)\n",
    "    stats_2_3.append(stat_2_3)\n",
    "\n",
    "    hr_3_end.append(df_3_end)\n",
    "    stats_3_end.append(stat_3_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(hr_all_part, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_val_result = pd.concat(hr_ob_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_1_2_result = pd.concat(stats_1_2)\n",
    "stat_2_3_result = pd.concat(stats_2_3)\n",
    "stat_3_end_result = pd.concat(stats_3_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_2 = pd.concat(hr_1_2, axis=1)\n",
    "df_2_3 = pd.concat(hr_2_3, axis=1)\n",
    "df_3_end = pd.concat(hr_3_end, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_val_result.to_csv(output_path+\"/HR_Under25_Over200_Values.csv\", float_format='%.6f')\n",
    "df_1_2.to_csv(output_path+\"/HR_1_2.csv\", float_format='%.6f')\n",
    "df_2_3.to_csv(output_path+\"/HR_2_3.csv\", float_format='%.6f')\n",
    "df_3_end.to_csv(output_path+\"/HR_3_end.csv\", float_format='%.6f')\n",
    "result.to_csv(output_path+\"/HR_All_Data.csv\", float_format='%.6f')\n",
    "stat_1_2_result.to_csv(output_path+\"/HR_1_2_Stats.csv\", float_format='%.6f')\n",
    "stat_2_3_result.to_csv(output_path+\"/HR_2_3_Stats.csv\", float_format='%.6f')\n",
    "stat_3_end_result.to_csv(output_path+\"/HR_3_end_Stats.csv\", float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
